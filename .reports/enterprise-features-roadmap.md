# Kiki ä¼ä¸šçº§ Agent è„šæ‰‹æ¶åŠŸèƒ½è¡¥å……å»ºè®®

> åŸºäº `aold/ai-engineer-training2/` AI å·¥ç¨‹å¸ˆè®­ç»ƒè¥é¡¹ç›®åˆ†æ

**ç”Ÿæˆæ—¥æœŸ**: 2026-02-03
**åˆ†æèŒƒå›´**: ai-engineer-training2/ å…¨æ ˆ 11 å‘¨è¯¾ç¨‹å†…å®¹

---

## ğŸ” ç‰ˆæœ¬å…¼å®¹æ€§åˆ†æ

| åŒ… | Kiki ç‰ˆæœ¬ | è®­ç»ƒè¥ç‰ˆæœ¬ | å…¼å®¹æ€§ |
|---|-----------|------------|--------|
| `langchain` | >= 1.2.7 | 0.3.27 | âš ï¸ **API å·®å¼‚** |
| `langchain-core` | >= 0.3.0 | 0.3.72 / 1.0.2 | âœ… å…¼å®¹ |
| `langgraph` | >= 0.3.0 | 0.6.4 | âœ… å…¼å®¹ |
| `langgraph-checkpoint-postgres` | >= 2.0.0 | 2.1.1 | âœ… å…¼å®¹ |

### âš ï¸ LangChain 1.x è¿ç§»æ³¨æ„äº‹é¡¹

**1. çŠ¶æ€å®šä¹‰é£æ ¼å˜åŒ–**

```python
# è®­ç»ƒè¥ä»£ç ï¼ˆæ—§é£æ ¼ï¼Œä»å…¼å®¹ï¼‰
from typing import TypedDict
class GenerationState(TypedDict):
    original_text: str
    chunks: list[str]

# Kiki æ¨èé£æ ¼ï¼ˆLangChain 1.x æœ€ä½³å®è·µï¼‰
from typing import Annotated
from langgraph.graph import add_messages
class GenerationState(TypedDict):
    messages: Annotated[list, add_messages]
    original_text: str
    chunks: list[str]
```

**2. ChatModel å¯¼å…¥è·¯å¾„**

```python
# è®­ç»ƒè¥ä»£ç 
from langchain_community.chat_models.tongyi import ChatTongyi

# Kiki æŠ½è±¡ï¼ˆæ¨èï¼‰
from app.llm import get_llm_service
llm = get_llm_service().get_chat_model()
```

**3. å·¥å…·ç»‘å®šæ–¹å¼**

```python
# è®­ç»ƒè¥ä»£ç ï¼ˆä»å…¼å®¹ï¼‰
model_with_tools = model.bind_tools([tool1, tool2])

# Kiki æ¨èæ–¹å¼
from app.agent.tools import alist_tools
llm_with_tools = llm_service.get_llm_with_tools()
```

### âœ… å¯ç›´æ¥å¤ç”¨çš„ä»£ç æ¨¡å¼

ä»¥ä¸‹æ¨¡å¼åœ¨ä¸¤ä¸ªç‰ˆæœ¬é—´å®Œå…¨å…¼å®¹ï¼š

| æ¨¡å¼ | è®­ç»ƒè¥ä»£ç  | Kiki å…¼å®¹æ€§ |
|------|-----------|-------------|
| `StateGraph` æ„å»º | `StateGraph(State)` | âœ… ç›´æ¥å…¼å®¹ |
| `ToolNode` | `ToolNode(tools)` | âœ… ç›´æ¥å…¼å®¹ |
| `START/END` | `from langgraph.graph import START, END` | âœ… ç›´æ¥å…¼å®¹ |
| çŸ¥è¯†å›¾è°± (NetworkX) | `networkx.MultiDiGraph` | âœ… çº¯ Pythonï¼Œæ— ä¾èµ– |
| `@tool` è£…é¥°å™¨ | `@tool def my_tool()` | âœ… ç›´æ¥å…¼å®¹ |

### ğŸ“ ä»£ç è¿ç§»æ¸…å•

ä»è®­ç»ƒè¥é¡¹ç›®è¿ç§»ä»£ç æ—¶ï¼Œéœ€è¦æ³¨æ„ï¼š

1. **æ›¿æ¢ LLM åˆå§‹åŒ–** â†’ ä½¿ç”¨ Kiki çš„ `LLMService`
2. **æ›¿æ¢é…ç½®è·å–** â†’ ä½¿ç”¨ `config["configurable"]`
3. **çŠ¶æ€å®šä¹‰é€‚é…** â†’ éµå¾ª Kiki çš„ `app.agent.state` æ¨¡å¼
4. **å·¥å…·æ³¨å†Œ** â†’ ä½¿ç”¨ Kiki çš„å·¥å…·æ³¨å†Œç³»ç»Ÿ

---

## ğŸ“Š Kiki å½“å‰åŠŸèƒ½çŸ©é˜µ

| æ¨¡å— | åŠŸèƒ½ | çŠ¶æ€ |
|------|------|------|
| **Agent æ ¸å¿ƒ** | BaseAgentã€ChatAgentã€ReactAgent | âœ… å®Œæ•´ |
| **å¤š Agent** | Supervisorã€Router æ¨¡å¼ | âœ… å®Œæ•´ |
| **è®°å¿†ç®¡ç†** | çŸ­æœŸè®°å¿†ã€é•¿æœŸè®°å¿†ï¼ˆåŸºç¡€ç‰ˆï¼‰ | âš ï¸ ç¼ºçŸ¥è¯†å›¾è°± |
| **é‡è¯•æœºåˆ¶** | æŒ‡æ•°é€€é¿ã€ç­–ç•¥é…ç½® | âœ… å®Œæ•´ |
| **å·¥å…·ç³»ç»Ÿ** | å·¥å…·æ³¨å†Œã€MCP é›†æˆã€æ‹¦æˆªå™¨ | âœ… å®Œæ•´ |
| **æµå¼è¾“å‡º** | Token/äº‹ä»¶æµ | âœ… å®Œæ•´ |
| **Human-in-the-Loop** | ä¸­æ–­ã€å®¡æ‰¹ | âœ… å®Œæ•´ |
| **RAG** | âŒ **ç¼ºå¤±** | ğŸ”´ **æ ¸å¿ƒç¼ºå¤±** |
| **å·¥å…·ç›‘æ§** | âŒ **ç¼ºå¤±** | ğŸ”´ **æ ¸å¿ƒç¼ºå¤±** |
| **ç¼“å­˜å±‚** | âŒ ç¼ºå¤± | ğŸŸ¡ å¯é€‰ |
| **ELK æ—¥å¿—** | âŒ ç¼ºå¤± | ğŸŸ¡ å¯é€‰ |
| **Prometheus ç›‘æ§** | âš ï¸ åŸºç¡€æŒ‡æ ‡ | ğŸŸ¡ å¯æ‰©å±• |

---

## ğŸ¯ ä¼˜å…ˆçº§åˆ†çº§

### P0 - æ ¸å¿ƒç¼ºå¤±åŠŸèƒ½ï¼ˆå¿…é¡»è¡¥å……ï¼‰

#### 1. çŸ¥è¯†å›¾è°±è®°å¿†æ¨¡å—

**å‚è€ƒæ¥æº**: `aold/ai-engineer-training2/week07/p10-KnowledgeTripleMEM.py`

**ç¼ºå¤±åŸå› **: å½“å‰é•¿æœŸè®°å¿†ä»…æ”¯æŒå‘é‡æ£€ç´¢ï¼ŒçŸ¥è¯†å›¾è°±èƒ½æä¾›æ›´å¼ºçš„æ¨ç†èƒ½åŠ›

**å®ç°å»ºè®®**:
```python
# app/agent/memory/knowledge_graph.py
from typing import Any

class KnowledgeGraphMemory(BaseLongTermMemory):
    """çŸ¥è¯†å›¾è°±è®°å¿†

    æ”¯æŒä¸‰å…ƒç»„ (Subject, Predicate, Object) å­˜å‚¨ï¼Œ
    æ”¯æŒå®ä½“å…³ç³»æ¨ç†å’Œå›¾è°±éå†ã€‚
    """

    async def add_triple(
        self,
        subject: str,
        predicate: str,
        obj: str,
        metadata: dict[str, Any] | None = None,
    ) -> str:
        """æ·»åŠ ä¸‰å…ƒç»„"""

    async def search_entity(
        self,
        entity: str,
        depth: int = 2,
    ) -> list[dict[str, Any]]:
        """æœç´¢å®ä½“ç›¸å…³ä¸‰å…ƒç»„"""

    async def get_neighbors(
        self,
        entity: str,
        direction: Literal["in", "out", "both"] = "both",
    ) -> list[str]:
        """è·å–ç›¸é‚»å®ä½“"""
```

**æ•°æ®åº“**: ä½¿ç”¨ Neo4j æˆ– PostgreSQL + `age` æ‰©å±•

---

#### 2. å®Œæ•´ RAG æ¨¡å—

**å‚è€ƒæ¥æº**: `aold/ai-engineer-training2/week03/`, `homework_examples/week03-homework-2/`

**ç¼ºå¤±åŸå› **: ä¼ä¸šçº§ Agent å¿…éœ€çŸ¥è¯†åº“èƒ½åŠ›

**ç›®å½•ç»“æ„**:
```
app/rag/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ retriever.py       # æ£€ç´¢å™¨åŸºç±»
â”œâ”€â”€ vector_retriever.py # å‘é‡æ£€ç´¢
â”œâ”€â”€ hybrid_retriever.py # æ··åˆæ£€ç´¢ (BM25 + å‘é‡)
â”œâ”€â”€ reranker.py        # é‡æ’åº
â”œâ”€â”€ document.py        # æ–‡æ¡£å¤„ç†
â”œâ”€â”€ chunker.py         # æ™ºèƒ½åˆ‡ç‰‡
â”œâ”€â”€ store.py           # å‘é‡å­˜å‚¨æŠ½è±¡
â””â”€â”€ graph_rag.py       # GraphRAG å®ç°
```

**æ ¸å¿ƒåŠŸèƒ½**:
- å‘é‡æ•°æ®åº“æ”¯æŒ: Milvusã€Qdrantã€PGVector
- æ··åˆæ£€ç´¢: BM25 + å‘é‡æ£€ç´¢èåˆ
- æ™ºèƒ½åˆ‡ç‰‡: è¯­ä¹‰æ„ŸçŸ¥çš„æ–‡æ¡£åˆ†å—
- é‡æ’åº: Cohere Rerank æˆ– BGE Reranker

---

#### 3. å·¥å…·æ‰§è¡Œç›‘æ§

**å‚è€ƒæ¥æº**: `aold/ai-engineer-training2/week08/ollama-exporter-main/`

**ç¼ºå¤±åŸå› **: ç”Ÿäº§ç¯å¢ƒéœ€è¦ç›‘æ§å·¥å…·è°ƒç”¨å¥åº·åº¦

**å®ç°å»ºè®®**:
```python
# app/agent/observability/tool_monitor.py
from prometheus_client import Counter, Histogram

tool_calls_total = Counter(
    "tool_calls_total",
    "Total tool calls",
    ["tool_name", "status"]
)

tool_duration = Histogram(
    "tool_duration_seconds",
    "Tool execution duration",
    ["tool_name"]
)

class ToolMonitor:
    """å·¥å…·æ‰§è¡Œç›‘æ§"""

    async def monitor_execution(
        self,
        tool_name: str,
        coro: Coroutine,
    ) -> Any:
        """ç›‘æ§å·¥å…·æ‰§è¡Œ"""
```

---

### P1 - é«˜ä»·å€¼å¢å¼º

#### 4. GraphRAG å®ç°

**å‚è€ƒæ¥æº**: `aold/ai-engineer-training2/homework_examples/week03-homework-2/graph_rag/`

**åŠŸèƒ½**: ç»“åˆçŸ¥è¯†å›¾è°±å’Œå‘é‡æ£€ç´¢ï¼Œæä¾›æ›´å¥½çš„çŸ¥è¯†æ¨ç†

```python
# app/rag/graph_rag.py
class GraphRAGRetriever:
    """GraphRAG æ£€ç´¢å™¨

    1. å‘é‡æ£€ç´¢è·å–å€™é€‰æ–‡æ¡£
    2. çŸ¥è¯†å›¾è°±æ‰©å±•ç›¸å…³å®ä½“
    3. å›¾è°±éå†å‘ç°éšå¼å…³è”
    4. èåˆæ’åºè¿”å›ç»“æœ
    """
```

---

#### 5. RAG è¯„ä¼°æ¡†æ¶

**å‚è€ƒæ¥æº**: `aold/ai-engineer-training2/week03/code/P32-ragas.py`

**åŠŸèƒ½**: ä½¿ç”¨ RAGAS é‡åŒ–è¯„ä¼° RAG ç³»ç»Ÿè´¨é‡

```python
# app/rag/evaluation.py
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
)

async def evaluate_rag_system(
    dataset: list[dict],
) -> dict[str, float]:
    """è¯„ä¼° RAG ç³»ç»Ÿæ€§èƒ½

    è¿”å›æŒ‡æ ‡:
    - faithfulness: å¿ å®åº¦
    - answer_relevancy: ç­”æ¡ˆç›¸å…³æ€§
    - context_precision: ä¸Šä¸‹æ–‡ç²¾ç¡®åº¦
    - context_recall: ä¸Šä¸‹æ–‡å¬å›ç‡
    """
```

---

#### 6. Redis ç¼“å­˜å±‚

**å‚è€ƒæ¥æº**: `aold/ai-engineer-training2/week09/3/p30ç¼“å­˜ç­–ç•¥è®¾è®¡/`

**åŠŸèƒ½**: å‡å°‘ LLM è°ƒç”¨ï¼Œæå‡å“åº”é€Ÿåº¦

```python
# app/infra/cache.py
from redis.asyncio import Redis
from typing import Callable

class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨"""

    async def get_or_compute(
        self,
        key: str,
        compute_fn: Callable,
        ttl: int = 3600,
    ) -> Any:
        """è·å–æˆ–è®¡ç®—ç¼“å­˜"""
```

---

#### 7. å¤šè¿›ç¨‹ + åç¨‹æ··åˆæ¶æ„

**å‚è€ƒæ¥æº**: `aold/ai-engineer-training2/week09/p21_å¤šè¿›ç¨‹ä¸åç¨‹æ··åˆ/`

**åŠŸèƒ½**: æ˜¾è‘—æå‡å¹¶å‘æ€§èƒ½

```python
# app/concurrency/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ scheduler.py      # ä»»åŠ¡è°ƒåº¦å™¨
â”œâ”€â”€ worker_pool.py    # å·¥ä½œè¿›ç¨‹æ± 
â””â”€â”€ async_bridge.py   # å¼‚æ­¥æ¡¥æ¥
```

---

### P2 - ç”Ÿäº§çº§å·¥ç¨‹åŒ–

#### 8. ELK æ—¥å¿—é›†æˆ

**å‚è€ƒæ¥æº**: `aold/ai-engineer-training2/week08/p41elk.py`

```python
# app/observability/elk.py
import structlog
from elasticsearch import AsyncElasticsearch

class ElasticsearchHandler:
    """Elasticsearch æ—¥å¿—å¤„ç†å™¨"""

    async def emit(self, log_dict: dict) -> None:
        """å‘é€æ—¥å¿—åˆ° Elasticsearch"""
```

---

#### 9. Prometheus æŒ‡æ ‡å¢å¼º

**å‚è€ƒæ¥æº**: `aold/ai-engineer-training2/week08/prometheus-config/`

```python
# app/observability/prometheus.py
from prometheus_client import Counter, Gauge, Histogram, Info

# LLM è°ƒç”¨æŒ‡æ ‡
llm_requests_total = Counter(...)
llm_tokens_total = Counter(...)
llm_latency = Histogram(...)

# Agent æŒ‡æ ‡
agent_iterations_total = Counter(...)
agent_errors_total = Counter(...)
```

---

#### 10. Celery å¼‚æ­¥ä»»åŠ¡

**å‚è€ƒæ¥æº**: `aold/ai-engineer-training2/week08/p17_webLLM/celery_app.py`

```python
# app/tasks/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ celery_app.py
â””â”€â”€ handlers/
    â”œâ”€â”€ document.py
    â””â”€â”€ rag_index.py
```

---

### P3 - é«˜çº§ç‰¹æ€§ï¼ˆå¯é€‰å¢å¼ºï¼‰

#### 11. æ¨¡å‹å¾®è°ƒæ”¯æŒ

**å‚è€ƒæ¥æº**: `aold/ai-engineer-training2/projects/project2_2/`

- LoRA å¾®è°ƒ: å‚æ•°é«˜æ•ˆå¾®è°ƒ
- æƒé‡åˆå¹¶: merge_and_unload ä¼˜åŒ–
- å¤šç»´è¯„ä¼°: ROUGEã€BERTScore

---

#### 12. DSL å·¥ä½œæµå¼•æ“

**å‚è€ƒæ¥æº**: `aold/ai-engineer-training2/week06/p15-CoffeeDSL/`

- DSL è¯­æ³•è§£æ: Lark è§£æå™¨é›†æˆ
- åŠ¨æ€è§„åˆ™ä¿®æ”¹
- SQL ç”Ÿæˆ DSL: Vanna è‡ªç„¶è¯­è¨€è½¬ SQL

---

## ğŸ“‹ å®æ–½è·¯çº¿å›¾

```
Phase 1 (P0) - æ ¸å¿ƒåŠŸèƒ½è¡¥é½
â”œâ”€â”€ çŸ¥è¯†å›¾è°±è®°å¿†æ¨¡å—
â”‚   â””â”€â”€ app/agent/memory/knowledge_graph.py
â”œâ”€â”€ å®Œæ•´ RAG æ¨¡å—
â”‚   â””â”€â”€ app/rag/
â””â”€â”€ å·¥å…·æ‰§è¡Œç›‘æ§
    â””â”€â”€ app/agent/observability/tool_monitor.py

Phase 2 (P1) - èƒ½åŠ›å¢å¼º
â”œâ”€â”€ GraphRAG
â”‚   â””â”€â”€ app/rag/graph_rag.py
â”œâ”€â”€ RAG è¯„ä¼°
â”‚   â””â”€â”€ app/rag/evaluation.py
â”œâ”€â”€ Redis ç¼“å­˜å±‚
â”‚   â””â”€â”€ app/infra/cache.py
â””â”€â”€ å¤šè¿›ç¨‹ + åç¨‹æ¶æ„
    â””â”€â”€ app/concurrency/

Phase 3 (P2) - ç”Ÿäº§çº§å·¥ç¨‹åŒ–
â”œâ”€â”€ ELK æ—¥å¿—é›†æˆ
â”‚   â””â”€â”€ app/observability/elk.py
â”œâ”€â”€ Prometheus ç›‘æ§å¢å¼º
â”‚   â””â”€â”€ app/observability/prometheus.py
â””â”€â”€ Celery å¼‚æ­¥ä»»åŠ¡
    â””â”€â”€ app/tasks/
```

---

## ğŸ“ å‚è€ƒä»£ç è·¯å¾„æ˜ å°„

| åŠŸèƒ½ | å‚è€ƒè·¯å¾„ | è¯´æ˜ |
|------|----------|------|
| **çŸ¥è¯†å›¾è°±è®°å¿†** | `aold/ai-engineer-training2/week07/p10-KnowledgeTripleMEM.py` | å®Œæ•´çš„çŸ¥è¯†å›¾è°±ç®¡ç†å™¨ |
| **GraphRAG** | `aold/ai-engineer-training2/homework_examples/week03-homework-2/graph_rag/` | Neo4j + å‘é‡æ£€ç´¢èåˆ |
| **æ··åˆæ£€ç´¢** | `aold/ai-engineer-training2/week03/code/P35-esæ··åˆæ£€ç´¢çš„å…¸å‹demo.ipynb` | BM25 + å‘é‡æ£€ç´¢ |
| **RAG è¯„ä¼°** | `aold/ai-engineer-training2/week03/code/P32-ragas.py` | RAGAS æ¡†æ¶é›†æˆ |
| **å·¥å…·é‡è¯•** | `aold/ai-engineer-training2/week07/p13-toolRetry.py` | æŒ‡æ•°é€€é¿é‡è¯•ç­–ç•¥ |
| **å¤šè¿›ç¨‹æ¶æ„** | `aold/ai-engineer-training2/week09/p21_å¤šè¿›ç¨‹ä¸åç¨‹æ··åˆ/` | æ··åˆå¹¶å‘æ¶æ„ |
| **ELK æ—¥å¿—** | `aold/ai-engineer-training2/week08/p41elk.py` | Logstash + ES é›†æˆ |
| **Prometheus** | `aold/ai-engineer-training2/week08/ollama-exporter-main/ollama_exporter.py` | Ollama æŒ‡æ ‡å¯¼å‡ºå™¨ |
| **Celery** | `aold/ai-engineer-training2/week08/p17_webLLM/celery_app.py` | å¼‚æ­¥ä»»åŠ¡é˜Ÿåˆ— |
| **Docker éƒ¨ç½²** | `aold/ai-engineer-training2/week08/docker/` | Docker Compose é…ç½® |
| **Kubernetes** | `aold/ai-engineer-training2/week08/p18_k8s/` | K8s éƒ¨ç½²é…ç½® |
| **æ¨¡å‹å¾®è°ƒ** | `aold/ai-engineer-training2/projects/project2_2/` | LoRA å¾®è°ƒå®Œæ•´å®ç° |

---

## ğŸ” è®­ç»ƒè¥é¡¹ç›®ç»“æ„æ¦‚è§ˆ

```
ai-engineer-training2/
â”œâ”€â”€ week01/          # LLM åŸºç¡€ä¸ LangGraph å…¥é—¨
â”œâ”€â”€ week02/          # æ¨¡å‹å¾®è°ƒ
â”œâ”€â”€ week03/          # LlamaIndex ä¸ RAG
â”œâ”€â”€ week04/          # LangChain å­¦ä¹ 
â”œâ”€â”€ week05/          # å¤š Agent åä½œ
â”œâ”€â”€ week06/          # DSL è¯­è¨€è®¾è®¡
â”œâ”€â”€ week07/          # æ™ºèƒ½ Agent é«˜çº§èƒ½åŠ›
â”œâ”€â”€ week08/          # å·¥ç¨‹åŒ–éƒ¨ç½²ä¸ç›‘æ§
â”œâ”€â”€ week09/          # Python é«˜æ€§èƒ½å¹¶å‘
â”œâ”€â”€ week10/          # ç»¼åˆå®æˆ˜é¡¹ç›®
â”œâ”€â”€ week11-homework/ # ç‹¼äººæ€æ¸¸æˆç³»ç»Ÿ
â”œâ”€â”€ homework_examples/ # ä¼˜ç§€ä½œä¸šç¤ºä¾‹
â””â”€â”€ projects/        # ç»¼åˆé¡¹ç›® (project1_1 ~ project5_2)
```

---

## ğŸ“ æŠ€æœ¯æ ˆå¯¹æ¯”

| å±‚çº§ | Kiki å½“å‰ | è®­ç»ƒè¥æ¨è | å»ºè®® |
|------|-----------|------------|------|
| **Agent æ¡†æ¶** | LangGraph | LangGraph | âœ… ä¿æŒ |
| **å‘é‡æ•°æ®åº“** | âŒ ç¼ºå¤± | Milvus, FAISS | ğŸ”´ éœ€è¡¥å…… |
| **çŸ¥è¯†å›¾è°±** | âŒ ç¼ºå¤± | Neo4j | ğŸ”´ éœ€è¡¥å…… |
| **æ—¥å¿—** | structlog | ELK | ğŸŸ¡ å¯å¢å¼º |
| **ç›‘æ§** | åŸºç¡€æŒ‡æ ‡ | Prometheus + Grafana | ğŸŸ¡ å¯å¢å¼º |
| **å¼‚æ­¥ä»»åŠ¡** | âŒ ç¼ºå¤± | Celery | ğŸŸ¡ å¯è¡¥å…… |
| **å¹¶å‘** | asyncio | å¤šè¿›ç¨‹ + åç¨‹ | ğŸŸ¡ å¯ä¼˜åŒ– |
| **éƒ¨ç½²** | âŒ ç¼ºå¤± | Docker + K8s | ğŸŸ¡ å¯è¡¥å…… |

---

## ğŸ“– LangChain 1.x ä»£ç é€‚é…æŒ‡å—

### çŸ¥è¯†å›¾è°±è®°å¿†æ¨¡å—ï¼ˆé€‚é… Kikiï¼‰

```python
# app/agent/memory/knowledge_graph.py
"""åŸºäºè®­ç»ƒè¥ä»£ç é€‚é…çš„çŸ¥è¯†å›¾è°±è®°å¿†æ¨¡å—"""

from __future__ import annotations

import json
import pickle
import uuid
from collections import defaultdict
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any

import networkx as nx
from langchain_core.tools import tool

from app.agent.memory.base import BaseLongTermMemory
from app.observability.logging import get_logger

logger = get_logger(__name__)


@dataclass
class KnowledgeNode:
    """çŸ¥è¯†èŠ‚ç‚¹"""
    label: str
    type: str
    properties: dict[str, Any] = field(default_factory=dict)
    id: str = field(default_factory=lambda: str(uuid.uuid4()))
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())


@dataclass
class KnowledgeTriple:
    """çŸ¥è¯†ä¸‰å…ƒç»„ (Subject, Predicate, Object)"""
    subject: str
    predicate: str
    object: str
    properties: dict[str, Any] = field(default_factory=dict)
    confidence: float = 1.0
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())


class KnowledgeGraphMemory(BaseLongTermMemory):
    """çŸ¥è¯†å›¾è°±è®°å¿† - Kiki é€‚é…ç‰ˆ

    åŸºäºè®­ç»ƒè¥ week07/p10-KnowledgeTripleMEM.py
    é€‚é…ç‚¹:
    - ä½¿ç”¨ BaseLongTermMemory æ¥å£
    - é›†æˆ Kiki æ—¥å¿—ç³»ç»Ÿ
    - æ”¯æŒä¾èµ–æ³¨å…¥
    """

    def __init__(
        self,
        storage_path: str | None = None,
        user_id: str | None = None,
    ) -> None:
        """åˆå§‹åŒ–çŸ¥è¯†å›¾è°±è®°å¿†

        Args:
            storage_path: å­˜å‚¨è·¯å¾„
            user_id: ç”¨æˆ· IDï¼ˆå¤šç§Ÿæˆ·éš”ç¦»ï¼‰
        """
        self.storage_path = Path(storage_path or "data/knowledge_graph")
        self.storage_path.mkdir(parents=True, exist_ok=True)

        self.user_id = user_id or "default"

        # æ•°æ®ç»“æ„
        self.nodes: dict[str, KnowledgeNode] = {}
        self.triples: list[KnowledgeTriple] = []
        self.nx_graph = nx.MultiDiGraph()

        # ç´¢å¼•
        self._node_label_index: defaultdict[str, set[str]] = defaultdict(set)

        self._load_or_create()

    async def add_memory(
        self,
        content: str,
        metadata: dict[str, Any] | None = None,
    ) -> str:
        """æ·»åŠ è®°å¿†ï¼ˆä¸‰å…ƒç»„å½¢å¼ï¼‰

        Args:
            content: ä¸‰å…ƒç»„ JSONï¼Œæ ¼å¼ {"subject": "...", "predicate": "...", "object": "..."}
            metadata: é¢å¤–å…ƒæ•°æ®

        Returns:
            è®°å¿† ID
        """
        try:
            data = json.loads(content) if isinstance(content, str) else content
            triple = KnowledgeTriple(
                subject=data["subject"],
                predicate=data["predicate"],
                object=data["object"],
                properties=metadata or {},
            )
            self.triples.append(triple)
            self._add_triple_to_graph(triple)
            self._save()
            logger.info(
                "knowledge_triple_added",
                subject=triple.subject,
                predicate=triple.predicate,
                object=triple.object,
            )
            return str(uuid.uuid4())

        except (KeyError, json.JSONDecodeError) as e:
            logger.error("invalid_triple_format", error=str(e))
            raise ValueError(f"æ— æ•ˆçš„ä¸‰å…ƒç»„æ ¼å¼: {e}") from e

    async def search_memories(
        self,
        query: str,
        k: int = 5,
        filter: dict[str, Any] | None = None,
    ) -> list[dict[str, Any]]:
        """æœç´¢è®°å¿†

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬ï¼ˆå®ä½“åç§°ï¼‰
            k: è¿”å›æ•°é‡
            filter: å…ƒæ•°æ®è¿‡æ»¤

        Returns:
            åŒ¹é…çš„ä¸‰å…ƒç»„åˆ—è¡¨
        """
        results = []

        # æŒ‰ä¸»ä½“æœç´¢
        for triple in self.triples:
            if query.lower() in triple.subject.lower():
                results.append({
                    "subject": triple.subject,
                    "predicate": triple.predicate,
                    "object": triple.object,
                    "confidence": triple.confidence,
                })
                if len(results) >= k:
                    break

        logger.debug("knowledge_search_completed", query=query, results=len(results))
        return results

    def _add_triple_to_graph(self, triple: KnowledgeTriple) -> None:
        """æ·»åŠ ä¸‰å…ƒç»„åˆ°å›¾"""
        # ç¡®ä¿èŠ‚ç‚¹å­˜åœ¨
        self._ensure_node(triple.subject)
        self._ensure_node(triple.object)

        # æ·»åŠ è¾¹
        source_id = self._get_node_id(triple.subject)
        target_id = self._get_node_id(triple.object)

        self.nx_graph.add_edge(
            source_id,
            target_id,
            relation=triple.predicate,
            confidence=triple.confidence,
        )

    def _ensure_node(self, label: str) -> None:
        """ç¡®ä¿èŠ‚ç‚¹å­˜åœ¨"""
        if label not in self._node_label_index:
            node = KnowledgeNode(label=label, type="entity")
            self.nodes[node.id] = node
            self._node_label_index[label.lower()].add(node.id)
            self.nx_graph.add_node(node.id, label=label)

    def _get_node_id(self, label: str) -> str | None:
        """è·å–èŠ‚ç‚¹ ID"""
        node_ids = self._node_label_index.get(label.lower(), set())
        return next(iter(node_ids)) if node_ids else None

    def _load_or_create(self) -> None:
        """åŠ è½½æˆ–åˆ›å»ºå­˜å‚¨"""
        data_file = self.storage_path / "graph_data.json"

        if data_file.exists():
            self._load()
        else:
            logger.info("creating_new_knowledge_graph", path=str(self.storage_path))

    def _load(self) -> None:
        """åŠ è½½ç°æœ‰æ•°æ®"""
        data_file = self.storage_path / "graph_data.json"

        try:
            with data_file.open("r", encoding="utf-8") as f:
                data = json.load(f)

            for triple_data in data.get("triples", []):
                triple = KnowledgeTriple(**triple_data)
                self.triples.append(triple)
                self._add_triple_to_graph(triple)

            logger.info("knowledge_graph_loaded", triples=len(self.triples))

        except (json.JSONDecodeError, KeyError) as e:
            logger.warning("knowledge_graph_load_failed", error=str(e))

    def _save(self) -> None:
        """ä¿å­˜æ•°æ®"""
        data_file = self.storage_path / "graph_data.json"

        data = {
            "triples": [
                {
                    "subject": t.subject,
                    "predicate": t.predicate,
                    "object": t.object,
                    "confidence": t.confidence,
                }
                for t in self.triples
            ],
        }

        with data_file.open("w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)


# ============== å·¥å…·å‡½æ•°ï¼ˆå…¼å®¹ Kiki å·¥å…·ç³»ç»Ÿï¼‰=============


def create_knowledge_graph_tools(memory: KnowledgeGraphMemory):
    """åˆ›å»ºçŸ¥è¯†å›¾è°±å·¥å…·é›†

    Args:
        memory: çŸ¥è¯†å›¾è°±è®°å¿†å®ä¾‹

    Returns:
        å·¥å…·åˆ—è¡¨
    """

    @tool
    async def add_knowledge_triple(
        subject: str,
        predicate: str,
        obj: str,
    ) -> str:
        """æ·»åŠ çŸ¥è¯†ä¸‰å…ƒç»„

        Args:
            subject: ä¸»ä½“
            predicate: è°“è¯/å…³ç³»
            obj: å®¢ä½“

        Returns:
            æ“ä½œç»“æœ
        """
        await memory.add_memory(
            content=json.dumps({"subject": subject, "predicate": predicate, "object": obj})
        )
        return f"âœ“ å·²æ·»åŠ ä¸‰å…ƒç»„: ({subject}, {predicate}, {obj})"

    @tool
    async def search_knowledge(entity: str) -> str:
        """æœç´¢çŸ¥è¯†å›¾è°±ä¸­çš„å®ä½“

        Args:
            entity: å®ä½“åç§°

        Returns:
            ç›¸å…³ä¸‰å…ƒç»„
        """
        results = await memory.search_memories(query=entity, k=5)

        if not results:
            return f"æœªæ‰¾åˆ°å…³äº '{entity}' çš„çŸ¥è¯†"

        lines = [f"å…³äº '{entity}' çš„çŸ¥è¯†:"]
        for r in results:
            lines.append(f"  â€¢ {r['subject']} --{r['predicate']}--> {r['object']}")

        return "\n".join(lines)

    return [add_knowledge_triple, search_knowledge]
```

### RAG æ¨¡å—åŸºç¡€æ¡†æ¶

```python
# app/rag/__init__.py
"""RAG æ¨¡å— - Kiki é€‚é…ç‰ˆ"""

from .retriever import BaseRetriever, VectorRetriever, HybridRetriever
from .store import VectorStore, create_vector_store
from .chunker import DocumentChunker, SemanticChunker

__all__ = [
    "BaseRetriever",
    "VectorRetriever",
    "HybridRetriever",
    "VectorStore",
    "create_vector_store",
    "DocumentChunker",
    "SemanticChunker",
]


# app/rag/store.py
"""å‘é‡å­˜å‚¨æŠ½è±¡"""

from abc import ABC, abstractmethod
from typing import Any

from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings


class VectorStore(ABC):
    """å‘é‡å­˜å‚¨æŠ½è±¡åŸºç±»"""

    @abstractmethod
    async def add_documents(self, documents: list[Document]) -> list[str]:
        """æ·»åŠ æ–‡æ¡£"""

    @abstractmethod
    async def similarity_search(
        self,
        query: str,
        k: int = 4,
        **kwargs: Any,
    ) -> list[Document]:
        """ç›¸ä¼¼åº¦æœç´¢"""


def create_vector_store(
    store_type: str = "faiss",
    embeddings: Embeddings | None = None,
    **kwargs: Any,
) -> VectorStore:
    """åˆ›å»ºå‘é‡å­˜å‚¨å®ä¾‹

    Args:
        store_type: å­˜å‚¨ç±»å‹ (faiss, milvus, pgvector)
        embeddings: åµŒå…¥æ¨¡å‹
        **kwargs: é¢å¤–å‚æ•°

    Returns:
        å‘é‡å­˜å‚¨å®ä¾‹
    """
    if store_type == "faiss":
        from .stores.faiss_store import FAISSVectorStore

        return FAISSVectorStore(embeddings=embeddings, **kwargs)

    # å…¶ä»–å­˜å‚¨ç±»å‹...
    raise ValueError(f"Unknown store type: {store_type}")


# app/rag/retriever.py
"""æ£€ç´¢å™¨å®ç°"""

from .store import VectorStore


class BaseRetriever(ABC):
    """æ£€ç´¢å™¨åŸºç±»"""

    @abstractmethod
    async def retrieve(self, query: str, k: int = 4) -> list[Document]:
        """æ£€ç´¢æ–‡æ¡£"""


class VectorRetriever(BaseRetriever):
    """å‘é‡æ£€ç´¢å™¨"""

    def __init__(self, vector_store: VectorStore) -> None:
        self.vector_store = vector_store

    async def retrieve(self, query: str, k: int = 4) -> list[Document]:
        return await self.vector_store.similarity_search(query, k=k)


class HybridRetriever(BaseRetriever):
    """æ··åˆæ£€ç´¢å™¨ (BM25 + å‘é‡)"""

    def __init__(
        self,
        vector_store: VectorStore,
        alpha: float = 0.5,  # èåˆæƒé‡
    ) -> None:
        self.vector_store = vector_store
        self.alpha = alpha

    async def retrieve(self, query: str, k: int = 4) -> list[Document]:
        # å…ˆåšå‘é‡æ£€ç´¢
        vector_results = await self.vector_store.similarity_search(query, k=k * 2)

        # TODO: åŠ å…¥ BM25 åˆ†æ•°èåˆ
        return vector_results[:k]
```

---

## æ€»ç»“

Kiki é¡¹ç›®å·²å…·å¤‡å®Œæ•´çš„ Agent æ ¸å¿ƒæ¡†æ¶ï¼Œä¸»è¦ç¼ºå¤± **RAG èƒ½åŠ›**å’Œ**çŸ¥è¯†å›¾è°±è®°å¿†**ä¸¤ä¸ªæ ¸å¿ƒä¼ä¸šçº§åŠŸèƒ½ã€‚

### å…³é”®é€‚é…è¦ç‚¹

1. **ç‰ˆæœ¬å…¼å®¹**: è®­ç»ƒè¥é¡¹ç›®ä½¿ç”¨ LangChain 0.3.x / 1.xï¼Œä¸ Kiki åŸºæœ¬å…¼å®¹
2. **LLM æŠ½è±¡**: ä½¿ç”¨ Kiki çš„ `LLMService` è€Œéç›´æ¥åˆå§‹åŒ–æ¨¡å‹
3. **çŠ¶æ€å®šä¹‰**: éµå¾ª Kiki çš„ `app.agent.state` æ¨¡å¼
4. **å·¥å…·æ³¨å†Œ**: ä½¿ç”¨ Kiki çš„å·¥å…·æ³¨å†Œç³»ç»Ÿ
5. **æ—¥å¿—é›†æˆ**: ä½¿ç”¨ `app.observability.logging.get_logger()`

å»ºè®®æŒ‰ P0 â†’ P1 â†’ P2 ä¼˜å…ˆçº§åˆ†é˜¶æ®µå®æ–½ï¼Œä¼˜å…ˆè¡¥é½æ ¸å¿ƒç¼ºå¤±åŠŸèƒ½ï¼Œå†é€æ­¥å¢å¼ºå·¥ç¨‹åŒ–èƒ½åŠ›ã€‚
